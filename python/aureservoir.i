/***************************************************************************/
/*!
 *  \file   aureservoir.i
 *
 *  \brief  SWIG interface to aureservoir library
 *
 *  \author Georg Holzmann, grh _at_ mur _dot_ at
 *  \date   Oct 2007
 *
 *   ::::_aureservoir_::::
 *   C++ library for analog reservoir computing neural networks
 *
 *   This library is free software; you can redistribute it and/or
 *   modify it under the terms of the GNU Lesser General Public
 *   License as published by the Free Software Foundation; either
 *   version 2.1 of the License, or (at your option) any later version.
 *
 ***************************************************************************/


/***************************************************************************/
// module docstring
%define DOCSTRING
"
aureservoir is an open-source (L-GPL) C++ library for analog
reservoir computing neural networks. The basic class is the ESN
(Echo State Network) class, which can be used in single or double
precision (SingleESN, DoubleESN).

Echo State Networks can be used with different initialization,
training and simulation algorithms - which can be choosen at runtime
from the definitions in the DATA section of the module documentation.
For more info on ESNs see docstrings of DoubleESN or SingleESN.

You can find autogenerated docstrings for most of the methods - for
more detailed documentation see the doxygen documentation of the
whole library at http://grh.mur.at (URL not yet valid).

2007, by
Georg Holzmann
grh _at_ mur _dot_ at
http://grh.mur.at
"
%enddef


/***************************************************************************/
// module definition

%module(docstring=DOCSTRING) aureservoir

%include numpy2carray.i

%{
#include "../aureservoir/aureservoir.h"
using namespace aureservoir;
%}

// general exception handling for AUExcept
%exception
{
   try {
      $action
   } catch (AUExcept &e) {
      PyErr_SetString( PyExc_RuntimeError, e.what().c_str() );
      return NULL;
   }
}


/***************************************************************************/
// C++ arrays to numpy conversions

ARRAY2_IN( float, float, FLOAT )
%apply (float *array, int rows, int cols)
{  (float *inmtx, int inrows, int incols),
   (float *outmtx, int outrows, int outcols),
   (float *wmtx, int wrows, int wcols)  };

ARRAY2_IN( double, double, DOUBLE )
%apply (double *array, int rows, int cols)
{  (double *inmtx, int inrows, int incols),
   (double *outmtx, int outrows, int outcols),
   (double *wmtx, int wrows, int wcols)  };

ARRAY1_IN( float, float, FLOAT )
%apply (float *array, int size)
{  (float *invec, int insize),
   (float *outvec, int outsize),
   (float *f1vec, int f1size),
   (float *f2vec, int f2size) };

ARRAY1_IN( double, double, DOUBLE )
%apply (double *array, int size)
{  (double *invec, int insize),
   (double *outvec, int outsize),
   (double *f1vec, int f1size),
   (double *f2vec, int f2size) };

ARRAY1_OUT( float*, FLOAT )
%apply (float ** array, int *size)
{ (float **vec, int *length) };

ARRAY1_OUT( double*, DOUBLE )
%apply (double ** array, int *size)
{ (double **vec, int *length) };

FARRAY2_OUT( float*, FLOAT )
%apply (float ** array, int *rows, int *cols)
{ (float **mtx, int *rows, int *cols) };

FARRAY2_OUT( double*, DOUBLE )
%apply (double ** array, int *rows, int *cols)
{ (double **mtx, int *rows, int *cols) };


/***************************************************************************/
// documentation

// use autodoc
%feature("autodoc", "1");

// include doxygen documentation
%include ESN.i

// so gehts (ohne namespace!):
// %feature("docstring", "TTTTTTTTTTTTTTTTTTTT")  ESN::simulate;


/***************************************************************************/
// class declarations
// NOTE: don't add throw(AUExcept) to the declarations !

template <typename T>
class ESN
{
 public:
  ESN();
  ESN(const ESN<T> &src);
  ~ESN();

  void init();
  void resetState();
  inline void train(T *inmtx, int inrows, int incols,
                    T *outmtx, int outrows, int outcols,
                    int washout);
  inline void simulate(T *inmtx, int inrows, int incols,
                       T *outmtx, int outrows, int outcols);
  inline void simulateStep(T *invec, int insize, T *outvec, int outsize);
  void setBPCutoff(T *f1vec, int f1size, T *f2vec, int f2size);

  void post();
  int getSize();
  int getInputs();
  int getOutputs();
  double getNoise();
  T getInitParam(InitParameter key);
  InitAlgorithm getInitAlgorithm();
  TrainAlgorithm getTrainAlgorithm();
  SimAlgorithm getSimAlgorithm();
  ActivationFunction getReservoirAct();
  ActivationFunction getOutputAct();

  void getWin(T **mtx, int *rows, int *cols);
  void getWback(T **mtx, int *rows, int *cols);
  void getWout(T **mtx, int *rows, int *cols);
  void getX(T **vec, int *length);
  void getW(T *wmtx, int wrows, int wcols);

  void setInitAlgorithm(InitAlgorithm alg=INIT_STD);
  void setTrainAlgorithm(TrainAlgorithm alg=TRAIN_LEASTSQUARE);
  void setSimAlgorithm(SimAlgorithm alg=SIM_STD);
  void setSize(int neurons=10);
  void setInputs(int inputs=1);
  void setOutputs(int outputs=1);
  void setNoise(double noise);
  void setInitParam(InitParameter key, T value);
  void setReservoirAct(ActivationFunction f=ACT_TANH);
  void setOutputAct(ActivationFunction f=ACT_LINEAR);

  void setWin(T *inmtx, int inrows, int incols);
  void setW(T *inmtx, int inrows, int incols);
  void setWback(T *inmtx, int inrows, int incols);
  void setWout(T *inmtx, int inrows, int incols);
  void setX(T *invec, int insize);
};

%template(DoubleESN) ESN<double>;
%template(SingleESN) ESN<float>;


/***************************************************************************/
// additional enums

enum InitParameter
{
  CONNECTIVITY,     //!< connectivity of the reservoir weight matrix
  ALPHA,            //!< spectral radius of the reservoir weight matrix
  IN_CONNECTIVITY,  //!< connectivity of the input weight matrix
  IN_SCALE,         //!< scale input weight matrix random vaules
  IN_SHIFT,         //!< shift input weight matrix random vaules
  FB_CONNECTIVITY,  //!< connectivity of the feedback weight matrix
  FB_SCALE,         //!< scale feedback weight matrix random vaules
  FB_SHIFT,         //!< shift feedback weight matrix random vaules
  LEAKING_RATE,     //!< leaking rate for Leaky Integrator ESNs
  TIKHONOV_FACTOR,  //!< regularization factor for TrainRidgeReg
  BP_F1,            //!< lowpass cutoff freq for bandpass style neurons
  BP_F2             //!< highpass cutoff freq for bandpass style neurons
};

enum InitAlgorithm
{
  INIT_STD,
  INIT_BP_CONST
};

enum SimAlgorithm
{
  SIM_STD,    //!< standard simulation \sa class SimStd
  SIM_SQUARE, //!< additional squared state updates \sa class SimSquare
  SIM_LI,     //!< simulation with leaky integrator neurons \sa class SimLI
  SIM_BP      //!< simulation with bandpass neurons \sa class SimBP
};

enum TrainAlgorithm
{
  TRAIN_PI,        //!< offline, pseudo inverse based \sa class TrainPI
  TRAIN_LS,        //!< offline least square algorithm, \sa class TrainLS
  TRAIN_RIDGEREG,  //!< with ridge regression, \sa class TrainRidgeReg
  TRAIN_PI_SQUARE
};

enum ActivationFunction
{
  ACT_LINEAR,      //!< linear activation function
  ACT_TANH         //!< tanh activation function
};
